{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"NLP_part4.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Yqk2pQcKLesH","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1603433976267,"user_tz":-480,"elapsed":1598,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"84094a20-ebbe-4474-9b0c-cd728a43bf08"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HKHZR7N4a1S_"},"source":["* @file NLP基礎/NLP_part4.ipynb\n","  * @brief NLP基礎 模型實作 \n","\n","  * 此份程式碼是以教學為目的，附有完整的架構解說。\n","\n","  * @author 人工智慧科技基金會 AI 工程師 - 康文瑋\n","  * Email: run963741@aif.tw\n","  * Resume: https://www.cakeresume.com/run963741\n","\n","  * 最後更新日期: 2020/11/26"]},{"cell_type":"markdown","metadata":{"id":"HHPchaQZ6hhe"},"source":["# 過去與展望\n","\n","傳統的自然語言處理都是透過統計方法將一個句子用各式各樣的統計量表示，例如詞頻、BOW、tf-idf 等等，但是這些統計量都沒有辦法真正表示詞的意義，下圖分別為三個自然語言處理領域常見的應用，這些任務都是傳統基於統計方法難以解決的問題，對於一詞多義這種情況，就無法用簡單統計量來表示。\n","\n","<figure>\n","<center>\n","<img src='https://drive.google.com/uc?export=view&id=1Ft8TZPXM_JjB9Jkaj61gzzAvaeaoN-2f' width=\"500\"/>\n","<figcaption>Parser, word Sense Disambiguation and coreference resolution</figcaption></center>\n","</figure>\n"]},{"cell_type":"markdown","metadata":{"id":"aDy3wWqkPv99"},"source":["# 載入套件以及資料集"]},{"cell_type":"code","metadata":{"id":"UuU6K6DF6hhl"},"source":["import pickle\n","from gensim.models import word2vec\n","import random\n","import logging\n","import tqdm\n","import os\n","\n","os.chdir('/content/drive/Shared drives/類技術班教材/標準版/NLP基礎')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eGL29jnu6hhs"},"source":["with open('Data/htl_cutted.pickle', 'rb') as file:\n","    data = pickle.load(file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qpa-yNIBYm6I","colab":{"base_uri":"https://localhost:8080/","height":598},"executionInfo":{"status":"ok","timestamp":1603433980044,"user_tz":-480,"elapsed":1465,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"46ec4a1e-0e37-411f-9d67-2bd7aeb7eb3c"},"source":["data[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['距離',\n"," '川沙',\n"," '公路',\n"," '較',\n"," '近',\n"," ',',\n"," '但是',\n"," '公交',\n"," '指示',\n"," '不',\n"," '對',\n"," ',',\n"," '如果',\n"," '是',\n"," '\"',\n"," '蔡陸線',\n"," '\"',\n"," '的話',\n"," ',',\n"," '會',\n"," '非常',\n"," '麻煩',\n"," '.',\n"," '建議',\n"," '用',\n"," '別的',\n"," '路線',\n"," '.',\n"," '房間',\n"," '較',\n"," '爲',\n"," '簡單',\n"," '.']"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"dL9vOBzxZxyM"},"source":["# Label encoding\n","\n","\b近年來有許多研究都聚焦在如何將詞用另一種數學形式來表示，第一種作法是 label encoding，簡單來說就是每個詞用一個數字來表示。\n","\n","這種定義方式最大的缺點在於大部分的詞之間本身是沒有大小關係的，例如下面的程式碼將 Taiwan 定義為 `2`，將 Australia  定義為 `0`，但這樣會導致 2 > 0 $\\Rightarrow$ Taiwan > Australia，但是國家本身是沒有大小之分的。"]},{"cell_type":"code","metadata":{"id":"oalV9SaVQMNm"},"source":["from sklearn.preprocessing import LabelEncoder\n","\n","labelencoder = LabelEncoder()\n","country = ['Taiwan','Australia','Ireland','Australia','Ireland','Taiwan']\n","\n","encode = labelencoder.fit_transform(country)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G6kBbNT-QMLd","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1603433981401,"user_tz":-480,"elapsed":2126,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"e236667e-7c25-4ca0-fa07-8d36b304d10a"},"source":["print('Input: \\n', country)\n","print('Labelencoder: \\n', encode)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input: \n"," ['Taiwan', 'Australia', 'Ireland', 'Australia', 'Ireland', 'Taiwan']\n","Labelencoder: \n"," [2 0 1 0 1 2]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cAIIAjnVR55s"},"source":["# One-hot encoding\n","\n","另一種方式是 One-hot encoding，也就是將每個詞用一個向量來表示，除了自己位置的數值是 1 之外，其餘都是 0，以下圖為例，顏色總共有 3 種，透過 one-hot encoding 轉換之後，每筆資料就會轉換成三維向量，以第一筆資料 Red 為例，就會轉換成 `[1,0,0]` 的向量，第一個位置為 1 表示紅色，其餘顏色的位置就為 0。\n","\n","<figure>\n","<center>\n","<img src='https://drive.google.com/uc?export=view&id=1U8aE8cVobe5M_N3aAtYJyf_6b_XiS-My' width=\"500\"/>\n","<figcaption>One-hot encoding</figcaption></center>\n","</figure>\n","\n","Resource: https://www.r-bloggers.com/2019/07/how-to-use-recipes-package-from-tidymodels-for-one-hot-encoding-%F0%9F%9B%A0-2/\n","\n","One-hot encoding 在特徵工程常常使用到，例如在 Dataframe 中常常看見文字需要轉換成 one-hot 再丟給模型或是在分類模型時常常使用在標籤轉換上。\n","\n","One-hot encoding 有兩個致命的問題，分別是線性獨立 (linear independent) 以及維度災難 (curse of dimensionality)：\n","\n","* 線性獨立 (linear independent):\n","\n","以上圖的例子來說，Red 和 Yellow 分別會用 `[1,0,0]` 和 `[0,1,0]` 來表示，若將這兩個向量使用內積相乘會得到 0，在數學上就表示這兩個向量彼此線性獨立，但是在生活中這兩種顏色本身是有一定的相關性的，所以 one-hot encoding 這種轉換方式對於文字來說是不夠好的。\n","\n","* 維度災難 (curse of dimensionality):\n","\n","細心的同學應該有發現，透過 one-hot 轉換後，向量的長度會跟字典的大小一樣 (以上圖的例子就是 3)，當字典大小會跟向量長度相同。以中文文本來說，字典大小約在 50,000 左右，這表示每個詞都會用 50,000 維的向量來表示，過高維度的資料會導致模型訓練效率急速下降，而且記憶體大小也有限制，例如以字典大小為 50,000，在記憶體就必須有位置讓你放 $50,000\\times 50,000$ 維矩陣，而且大部分的位置都是 0 (sparse matrix)，這樣很沒有效率。"]},{"cell_type":"code","metadata":{"id":"asbQ3BX8QMJg"},"source":["from sklearn.preprocessing import OneHotEncoder\n","\n","country_idx = [['Taiwan'],['Australia'],['Ireland'],['Japan']]\n","\n","enc = OneHotEncoder()\n","onehot = enc.fit_transform(country_idx).toarray()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vqGh6MS5S_qj","colab":{"base_uri":"https://localhost:8080/","height":140},"executionInfo":{"status":"ok","timestamp":1603433981404,"user_tz":-480,"elapsed":1649,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"4371861b-6753-4c37-afa3-ed754134476b"},"source":["print('Input: \\n', country_idx)\n","print('One-hot encoding: \\n', onehot)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input: \n"," [['Taiwan'], ['Australia'], ['Ireland'], ['Japan']]\n","One-hot encoding: \n"," [[0. 0. 0. 1.]\n"," [1. 0. 0. 0.]\n"," [0. 1. 0. 0.]\n"," [0. 0. 1. 0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wXLsfFyR17eY"},"source":["# Word2vec\n","\n","word2vec 的概念是於 2013 年在 [Distributed Representations of Words and Phrases\n","and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) 所提出，也是將每個字表示為一個向量，但是這個向量是用神經網路來訓練的，以下圖為例，將 man 用一個 7 維向量來表示，然後通過模型訓練後，再將 7 維向量降維然後畫在圖上，會形成類似下圖右邊坐標系的效果，從第一個坐標系來看，模型可以學習到男性跟女性之間的關係，也就是：\n","\n","$$\n","king - queen=man - woman\n","$$\n","\n","在第二個坐標系中，模型學習到了現在進行式與過去式之間的對應關係：\n","\n","$$\n","walked - walking = swam - swimming\n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"ac3AJxsV46Iw"},"source":["<figure>\n","<center>\n","<img src='https://drive.google.com/uc?export=view&id=10meJYPIlusaY4dm_Snfa_fFbxjq7vxSp' width=\"800\"/>\n","<figcaption>One-hot encoding</figcaption></center>\n","</figure>"]},{"cell_type":"markdown","metadata":{"id":"WKgO0xlx7OK7"},"source":["到目前為止，Word2vec 詞向量的概念基本上已經稱霸自然語言處理領域，許多知名演算法都是以詞向量作為模型輸入，那這些向量是怎麼被訓練出來的呢 ? 這邊介紹兩個知名的演算法，分別是 CBOW (Continuous bag of words) 以及 Skip-gram。"]},{"cell_type":"markdown","metadata":{"id":"9lxWpruB8EUO"},"source":["## CBOW (Continuous bag of words)\n","\n","CBOW 的做法類似克漏字測驗，也就是用上下文來預測中間的詞，若模型有能力預測出中間的詞，就表示模型有學習到詞之間的關係：\n","\n","<figure>\n","<center>\n","<img src='https://drive.google.com/uc?export=view&id=1CjMp04zT1Jlh_iK3sMVCydkShCPD0FH2' width=\"500\"/>\n","<figcaption>CBOW</figcaption></center>\n","</figure>\n","\n","\n","第一步：首先將每個詞轉成 one-hot encoding，然後上下文共 $C$ 個 one-hot 做總和，然後再通過矩陣 $W\n","in R^{V\\times N}$，接著再平均，得到 hidden representation $h$，這裡的權重 $W$ 就是由所有詞向量所構成的矩陣，也就是我們等一下要觀察的東西：\n","$$\n","h=\\frac{1}{C}W\\cdot(\\sum_{i=1}^Cx_i)\n","$$\n","\n","\n","第二步：將 $h$ 乘以權重 $W'\\in R^{N\\times V}$，$V$ 指的是字典大小：\n","\n","$$\n","Y=W'^T\\cdot h,\\;Y\\in R^V\n","$$\n","\n","\n","第三步：將預測值 $Y$ 通過 softmax 得到每個詞的概率值，接著就能夠計算 loss 並\n","更新權重：\n","\n","$$\n","softmax(Y)=\\frac{\\exp(Y_i)}{\\sum_{k=1}^V\\exp(Y_k)}\n","$$\n","\n","<figure>\n","<center>\n","<img src='https://drive.google.com/uc?export=view&id=1yCedxszktzJwLbVvwo5fgJKYD3lLy3Ct' width=\"500\"/>\n","<figcaption>CBOW</figcaption></center>\n","</figure>\n"]},{"cell_type":"code","metadata":{"id":"z0nWTdgE6hhu"},"source":["# https://radimrehurek.com/gensim/models/word2vec.html\n","# sg=0 CBOW ; sg=1 skip-gram\n","# size: 詞向量維度\n","# min_count: 頻率大於等於 3 才會作為 word2vec 的字典使用\n","# window: cbow 模型底下一次取多少詞來預測中間的詞\n","bow_model = word2vec.Word2Vec(size=128, min_count=3, window=5, sg=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"ZsYEz4I66hhy"},"source":["# 首先建立字典\n","bow_model.build_vocab(data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"SVFZQxAq6hh5","colab":{"base_uri":"https://localhost:8080/","height":90},"executionInfo":{"status":"ok","timestamp":1603434010692,"user_tz":-480,"elapsed":19900,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"b40cd033-a481-4dbf-c1c6-575aaf360c29"},"source":["# train word2vec model ; shuffle data every epoch\n","for i in tqdm.tqdm(range(20)):\n","    random.shuffle(data)\n","    bow_model.train(data, total_examples=len(data), epochs=2)\n","\n","bow_model.save(\"word2vec_model/cbow.model\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 20/20 [00:18<00:00,  1.07it/s]\n","/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Zf4xSeTKQ8uA","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1603434012317,"user_tz":-480,"elapsed":1603,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"576f3d74-9189-4c40-8c0c-01e1cd6bc4a1"},"source":["# 讀取模型\n","bow_model = word2vec.Word2Vec.load(\"word2vec_model/cbow.model\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"6IeN9Tdd6hiA","colab":{"base_uri":"https://localhost:8080/","height":474},"executionInfo":{"status":"ok","timestamp":1603434012318,"user_tz":-480,"elapsed":1579,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"329bc3f5-f3d1-46f2-be45-5c168ecae5b7"},"source":["# 觀察 飯店 的詞向量\n","bow_model.wv['飯店']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-1.0701672 , -0.5366705 , -0.14141004,  1.4369427 , -0.7404118 ,\n","        0.5987442 ,  1.1751707 ,  0.49842593,  1.5675281 , -0.02220872,\n","        1.1056122 ,  0.61487997, -1.7171193 , -1.9098822 , -0.25944138,\n","        1.1256552 , -0.16464075,  0.5829185 , -1.4798691 , -0.44180414,\n","       -0.02606585,  1.9827013 ,  0.15339974,  0.38701537, -0.11875502,\n","       -0.62823737, -0.9901219 , -1.8006724 ,  2.3847196 , -0.66525346,\n","       -0.64452624, -2.0062046 ,  1.1393052 ,  3.2320094 , -0.1776061 ,\n","       -1.0081499 , -0.36308467, -2.11115   , -0.09659322, -0.14233512,\n","        1.1370134 ,  0.08736645, -2.405578  ,  0.9725603 , -1.3069246 ,\n","       -0.48618224,  0.6675372 ,  0.1661223 , -0.62522864, -0.68891925,\n","        0.6024552 ,  1.5583718 ,  0.15468223, -0.4883932 , -0.26298532,\n","       -1.2618384 , -3.9300814 , -0.52221674, -2.6756866 ,  1.3235472 ,\n","       -2.1499412 , -0.9530751 , -0.6591422 ,  1.8895234 ,  0.40236062,\n","       -0.83020693,  1.0518736 ,  0.83717227,  0.477712  , -0.5269778 ,\n","       -1.5756942 ,  1.645418  , -0.29038483, -0.39759454, -0.78186816,\n","       -1.3464669 ,  0.13982245,  0.6632617 ,  1.2203536 ,  2.0828955 ,\n","       -1.1697503 ,  0.6525414 , -1.4600294 ,  3.108421  , -2.8851736 ,\n","       -2.0586822 ,  1.5136186 ,  0.3278403 , -1.1905421 ,  0.05045517,\n","        1.3721912 , -2.070501  ,  1.0452282 , -0.1134304 ,  2.2410653 ,\n","        0.20750633,  1.0419537 ,  0.6077134 , -0.7270889 ,  1.8260355 ,\n","       -2.793158  ,  1.7195761 , -0.94593877,  0.9438348 , -2.3119645 ,\n","        0.90697986, -0.69436234, -0.7174458 , -1.8309501 ,  1.0690696 ,\n","        2.7706416 ,  2.158272  ,  1.0190442 , -0.68569726,  2.1651552 ,\n","        0.65609556,  1.1846608 , -0.00717269, -0.15513638, -0.28423092,\n","        0.83534336,  2.289481  ,  0.25276384, -1.9028766 ,  1.1222523 ,\n","        1.9517456 ,  0.12500271,  1.0383126 ], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"GzLlYt8KZPeb","colab":{"base_uri":"https://localhost:8080/","height":248},"executionInfo":{"status":"ok","timestamp":1603434012319,"user_tz":-480,"elapsed":1562,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"00e2d023-380f-4caf-8e6d-b9e438da1468"},"source":["# 看看與 飯店 和字典中最接近的詞是什麼\n","bow_model.wv.most_similar('飯店')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n","  if np.issubdtype(vec.dtype, np.int):\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[('賓館', 0.6172376275062561),\n"," ('酒店', 0.5052703619003296),\n"," ('城市', 0.4633227586746216),\n"," ('南昌', 0.36634230613708496),\n"," ('商店', 0.3613012433052063),\n"," ('地區', 0.3563742935657501),\n"," ('早茶', 0.35457515716552734),\n"," ('小吃店', 0.3527277708053589),\n"," ('居民區', 0.34699302911758423),\n"," ('旅館', 0.34555113315582275)]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"JmrJq5Ma6hiG","colab":{"base_uri":"https://localhost:8080/","height":248},"executionInfo":{"status":"ok","timestamp":1603434012320,"user_tz":-480,"elapsed":1543,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"bf6695c6-9d64-4c35-8aa7-adc378c35671"},"source":["# 找相對應的詞，冬天 - 暖氣 + 夏天 = ?\n","bow_model.wv.most_similar(positive=['冬天', '暖氣'], negative=['夏天'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n","  if np.issubdtype(vec.dtype, np.int):\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[('製', 0.6001927256584167),\n"," ('空調', 0.5621954202651978),\n"," ('冷氣', 0.5377000570297241),\n"," ('暖風', 0.5338841676712036),\n"," ('水溫', 0.5200881958007812),\n"," ('溫度', 0.49906039237976074),\n"," ('熱', 0.4924931526184082),\n"," ('熱水', 0.48980918526649475),\n"," ('調節', 0.4820823669433594),\n"," ('開關', 0.4768384099006653)]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"UQbPRSC6QVzJ"},"source":["## Skip-gram \n","\n","Skip-gram 的預測目標與 CBOW 相反，是使用某一詞去預測上下文，以下圖為例，我們使用中間的詞 `很大, 記得帶一支`，來預測上下文，透過這種方式來訓練詞向量：\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xEftKhwmUQIh"},"source":["<figure>\n","<center>\n","<img src='https://drive.google.com/uc?export=view&id=15CCMlDIT41AvmcVki73ut4rG3EMncljv' width=\"700\"/>\n","<figcaption>Skip-gram</figcaption></center>\n","</figure>"]},{"cell_type":"markdown","metadata":{"id":"k6rmWR0XVuCf"},"source":["在開始訓練前，skip-gram 會假設一個 window_size，以下圖為例，window_size 為3，並使用中間的詞 `passes` 來預測上下文 `who` 以及 `the`。"]},{"cell_type":"markdown","metadata":{"id":"sdhnbquyVcHR"},"source":["<figure>\n","<center>\n","<img src='https://drive.google.com/uc?export=view&id=1ZMQ0m62jrYiIAxG8W0HE4HaUSexnOeAO' width=\"700\"/>\n","<figcaption>Skip-gram with window size equal to three</figcaption></center>\n","</figure>\n","\n","Resource: https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling\n"]},{"cell_type":"markdown","metadata":{"id":"_PrpaYoyWo9X"},"source":["假設 window size 為 5，則所有情況如下圖，也就是用 **Target word** 預測 **Context** 裡面的詞，以第一列 (row) 為例，就會產生 (the,man), (the,who) 這兩筆訓練樣本，也就是分別用 `the` 來預測 `man`，然後再用 `the` 來預測 `who`。"]},{"cell_type":"markdown","metadata":{"id":"5yja9d5rWbd8"},"source":["<figure>\n","<center>\n","<img src='https://drive.google.com/uc?export=view&id=1-3DEDTyB4YnG7yDMaAyzesWvmUCOHL89' width=\"700\"/>\n","<figcaption>Skip-gram with window size equal to five</figcaption></center>\n","</figure>\n","\n","Resource: https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#context-based-skip-gram-model"]},{"cell_type":"markdown","metadata":{"id":"eaZzMXUqXRF3"},"source":["Skip-gram 運作流程如下圖：\n","\n","第一步：將 `target` 轉換為 One-hot encoding $X\\in R^V$ ($V$ 為字典大小)，然後再乘以權重 $W\\in R^{V\\times N}$ ($N$ 為詞向量大小)，得到 hidden representation $h\\in R^N$：\n","\n","$$\n","h=XW\n","$$\n","\n","\n","第二步：將 hidden representation $h$ 乘以權重 $W'\\in R^{N\\times V}$，得到預測值 $Y\\in R^V$：\n","\n","$$\n","Y=hW'\n","$$\n","\n","\n","第三步：將預測值 $Y$ 通過 softmax 得到每個詞的概率值，接著就能夠計算 loss 並\n","更新權重：\n","\n","$$\n","softmax(Y)=\\frac{\\exp(Y_i)}{\\sum_{k=1}^V\\exp(Y_k)}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"is_DUCBKXRff"},"source":["<figure>\n","<center>\n","<img src='https://drive.google.com/uc?export=view&id=11YZ_IpprytY4ybGt85f7CabBDbUtKvug' width=\"700\"/>\n","<figcaption>Skip-gram pipeline</figcaption></center>\n","</figure>"]},{"cell_type":"code","metadata":{"id":"QLLfIKSqj3Hn"},"source":["# https://radimrehurek.com/gensim/models/word2vec.html\n","# sg=0 CBOW ; sg=1 skip-gram\n","# size: 詞向量維度\n","# min_count: 頻率大於等於 3 才會作為 word2vec 的字典使用\n","# window: cbow 模型底下一次取多少詞來預測中間的詞\n","skipgram_model = word2vec.Word2Vec(size=128, min_count=3, window=3, sg=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"62V3EKtxe9d2"},"source":["# 首先建立字典\n","skipgram_model.build_vocab(data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qQe1REQdfCZm","colab":{"base_uri":"https://localhost:8080/","height":90},"executionInfo":{"status":"ok","timestamp":1603434066169,"user_tz":-480,"elapsed":45865,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"b988cfbc-04c1-4dcc-baf3-edab5e0c7e50"},"source":["# train word2vec model ; shuffle data every epoch\n","for i in tqdm.tqdm(range(20)):\n","    random.shuffle(data)\n","    skipgram_model.train(data, total_examples=len(data), epochs=2)\n","\n","skipgram_model.save(\"word2vec_model/skipgram.model\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 20/20 [00:42<00:00,  2.13s/it]\n","/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"9Tuafm2qfJ-j","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1603434070030,"user_tz":-480,"elapsed":1009,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"7e55978f-9060-42ef-a282-48693d8c20f8"},"source":["# 讀取模型\n","skipgram_model = word2vec.Word2Vec.load(\"word2vec_model/skipgram.model\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"RUpeMmYPfKEQ","colab":{"base_uri":"https://localhost:8080/","height":474},"executionInfo":{"status":"ok","timestamp":1603434071504,"user_tz":-480,"elapsed":1016,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"df903bde-1352-4ebf-a86d-359493bc9eb6"},"source":["# 觀察 飯店 的詞向量\n","skipgram_model.wv['飯店']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-0.1921961 ,  0.6429113 ,  0.84826696, -0.17512208, -0.16462117,\n","       -0.18926574,  0.20201826, -0.21031773,  0.45067227,  0.03488045,\n","        0.29420573,  0.2214367 , -0.01396547, -0.68384683, -0.3797484 ,\n","       -0.07619353,  0.31956688,  0.12331374, -0.7047422 , -0.1487584 ,\n","        0.43465686,  0.2951043 ,  0.07627244,  0.252892  , -0.0542222 ,\n","       -0.3137878 , -0.23479237, -0.9533322 ,  0.5642962 ,  0.17222688,\n","        0.08840205, -0.28939658,  0.37507924, -0.01490949, -0.37481546,\n","       -0.71201515, -0.11033753, -0.4433349 ,  0.284574  ,  0.2573987 ,\n","        0.49753553, -0.4114739 , -0.57567847, -0.363803  , -0.239369  ,\n","       -0.06081809, -0.29769722,  0.09416001, -0.40276113, -0.41059715,\n","        0.3371592 ,  0.15318027,  0.189732  , -0.09726161,  0.06078521,\n","       -0.18523186, -0.791787  , -0.13431887, -0.8530894 , -0.26824167,\n","        0.10745721,  0.08118854,  0.12035467,  0.29548284,  0.23184474,\n","        0.10766387, -0.06128077,  0.00350666,  0.14721055,  0.27864215,\n","       -0.23404709, -0.44739977, -0.051155  ,  0.45464104, -0.05155448,\n","        0.4784272 ,  0.36597848,  0.07106713, -0.18629552,  0.52650887,\n","        0.15234807,  0.33697364,  0.05842954, -0.09060757, -0.05028306,\n","       -0.16289137,  0.0310461 ,  0.31781605,  0.39366323,  0.0482921 ,\n","       -0.08113836, -0.15549989,  0.2730101 , -0.0874686 ,  0.24156861,\n","        0.06507012,  0.3522113 ,  0.18557534,  0.23191386,  0.8117296 ,\n","        0.07971141, -0.05582038, -0.04915748,  0.01042728, -0.3147893 ,\n","       -0.13469182, -0.74580395, -0.10745834, -0.16095726,  0.13054997,\n","        0.39954102,  0.0983334 ,  0.16777092,  0.4822388 , -0.0074904 ,\n","        0.46603352, -0.00511728, -0.02110444,  0.30742   , -0.5412136 ,\n","        0.19893426,  0.20912969, -0.09102108, -0.24148554,  0.69583005,\n","        0.40944138,  0.292685  ,  0.01998366], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"vi0qql8FfKCJ","colab":{"base_uri":"https://localhost:8080/","height":248},"executionInfo":{"status":"ok","timestamp":1603434072733,"user_tz":-480,"elapsed":1013,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"94f125f8-65c0-49a7-a0f3-1214053348f0"},"source":["# 看看與 飯店 和字典中最接近的詞是什麼\n","skipgram_model.wv.most_similar('飯店')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n","  if np.issubdtype(vec.dtype, np.int):\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[('酒店', 0.5937623381614685),\n"," ('甜品店', 0.4943004250526428),\n"," ('賓館', 0.48139411211013794),\n"," ('旅社', 0.44420185685157776),\n"," ('唐山', 0.4416014850139618),\n"," ('西溪', 0.4403097629547119),\n"," ('揚子島', 0.43499356508255005),\n"," ('向來', 0.4346087574958801),\n"," ('王府', 0.42976114153862),\n"," ('市郊', 0.42503321170806885)]"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"Ftdzn9OIfTl5","colab":{"base_uri":"https://localhost:8080/","height":248},"executionInfo":{"status":"ok","timestamp":1603434076098,"user_tz":-480,"elapsed":1422,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"7b49765d-faf7-4f36-f650-501eced7624c"},"source":["# 找相對應的詞，冬天 - 暖氣 + 夏天 = ?\n","skipgram_model.wv.most_similar(positive=['冬天', '暖氣'], negative=['夏天'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n","  if np.issubdtype(vec.dtype, np.int):\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[('空調', 0.4979294240474701),\n"," ('製', 0.48356133699417114),\n"," ('褥子', 0.48000916838645935),\n"," ('tnnd', 0.4640786647796631),\n"," ('強勁', 0.45863544940948486),\n"," ('角房', 0.4522526264190674),\n"," ('充足', 0.4378127455711365),\n"," ('洗澡間', 0.4367930591106415),\n"," ('暖風', 0.4349173903465271),\n"," ('電話機', 0.4324340522289276)]"]},"metadata":{"tags":[]},"execution_count":22}]}]}